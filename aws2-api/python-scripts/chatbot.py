import re
import time
import json
import uuid
import boto3
import traceback
from datetime import datetime, timedelta, timezone
from functools import lru_cache
import concurrent.futures as _f
from typing import Optional, List, Dict, Tuple

# ===== ì„¤ì • =====
REGION = "ap-northeast-2"

# S3 (ë°ì´í„°/ë¡œê·¸ ë¶„ë¦¬)
S3_BUCKET_DATA = "aws2-airwatch-data"   # ì„¼ì„œ ë°ì´í„°ê°€ ë“¤ì–´ìˆëŠ” ë²„í‚· (ê¸°ì¡´)
CHATLOG_BUCKET = "chatlog-1293845"      # <-- ìš”ì²­í•˜ì‹  ì±„íŒ… ë¡œê·¸ ì €ì¥ìš© ë²„í‚·
S3_PREFIX = ""  # ë°ì´í„° í´ë” êµ¬ë¶„ì€ í‚¤ì—ì„œ ìë™ íŒë‹¨

# RAG/ê²€ìƒ‰
TOP_K = 8
LIMIT_CONTEXT_CHARS = 100000
MAX_FILES_TO_SCAN = 100000
MAX_WORKERS = 10
MAX_FILE_SIZE = 1024 * 1024  # 1MB
RELEVANCE_THRESHOLD = 1  # ë” ê´€ëŒ€í•œ ì„ê³„ê°’ìœ¼ë¡œ ì¡°ì •

# í•„ë“œ ë™ì˜ì–´/ë¼ë²¨
FIELD_SYNONYMS = {
    "ì˜¨ë„": "temperature", "temp": "temperature", "temperature": "temperature",
    "ìŠµë„": "humidity", "hum": "humidity", "humidity": "humidity",
    "ê³µê¸°ì§ˆ": "gas", "ê°€ìŠ¤": "gas", "gas": "gas", "ppm": "gas",
    "co2": "gas", "coâ‚‚": "gas", "ì´ì‚°í™”íƒ„ì†Œ": "gas"
}
FIELD_NAME_KOR = {"temperature": "ì˜¨ë„", "humidity": "ìŠµë„", "gas": "ì´ì‚°í™”íƒ„ì†Œ(CO2)"}

# Bedrock (Inference Profile ARN for Claude Sonnet 4)
INFERENCE_PROFILE_ARN = "arn:aws:bedrock:ap-northeast-2:070561229682:inference-profile/apac.anthropic.claude-sonnet-4-20250514-v1:0"

# ===== í´ë¼ì´ì–¸íŠ¸ =====
s3 = boto3.client("s3", region_name=REGION)           # ë°ì´í„° ì ‘ê·¼ìš©
s3_logs = boto3.client("s3", region_name=REGION)      # ë¡œê·¸ ì €ì¥ìš© (ë™ì¼ ë¦¬ì „)
bedrock_rt = boto3.client("bedrock-runtime", region_name=REGION)

# ===== ì‹œê°„ëŒ€ ë³´ì • (ë‚´ë¶€ ë¹„êµëŠ” 'KST naive') =====
KST = timezone(timedelta(hours=9))
def _to_kst_naive(dt: datetime) -> datetime:
    if dt.tzinfo:
        return dt.astimezone(KST).replace(tzinfo=None)
    return dt

# ===== í† í¬ë‚˜ì´ì € & ì •ê·œí™” =====
def tokenize(s: str):
    return re.findall(r"[A-Za-z0-9ê°€-í£_:+-]+", s.lower())

def normalize_query_tokens(q: str):
    tokens = tokenize(q)
    return [FIELD_SYNONYMS.get(t, t) for t in tokens]

def detect_fields_in_query(raw_query: str):
    q = raw_query.lower()
    fields = set()
    if ("ìŠµë„" in raw_query) or ("hum" in q) or ("humidity" in q): fields.add("humidity")
    if ("ì˜¨ë„" in raw_query) or ("temp" in q) or ("temperature" in q): fields.add("temperature")
    if ("ê³µê¸°ì§ˆ" in raw_query) or ("ê°€ìŠ¤" in raw_query) or ("gas" in q) or ("ppm" in q)or ("co2" in q) or ("coâ‚‚" in raw_query) or ("ì´ì‚°í™”íƒ„ì†Œ" in raw_query): fields.add("gas")
    return fields

def want_detail_list(query: str) -> bool:
    detail_words = ["ìƒì„¸", "ìì„¸íˆ", "ìì„¸í•˜ê²Œ", "ìƒì„¸íˆ", "ì›ë³¸", "ëª©ë¡"]
    q = query.strip()
    return any(word in q for word in detail_words)

# ===== ë‚ ì§œ/ì‹œê°„ íŒŒì‹± =====
ISO_PAT = r"\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:\.\d+)?(?:Z|[+-]\d{2}:\d{2})?"
def extract_datetime_strings(s: str):
    out = []
    out += re.findall(ISO_PAT, s)  # ISO8601
    patterns = [
        r"\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}",
        r"\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}",
        r"\d{4}-\d{2}-\d{2}"
    ]
    for p in patterns: out += re.findall(p, s)

    # ê°œì„ ëœ í•œêµ­ì–´ ë‚ ì§œ íŒŒì‹± - ì—¬ëŸ¬ íŒ¨í„´ ì§€ì› (ì˜¤ì „/ì˜¤í›„ í¬í•¨)
    korean_patterns = [
        # 2025ë…„ 8ì›” 11ì¼ 14ì‹œ 00ë¶„ 05ì´ˆ
        r"(\d{4})\s*ë…„\s*(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼\s*(\d{1,2})\s*ì‹œ\s*(\d{1,2})\s*ë¶„\s*(\d{1,2})\s*ì´ˆ",
        # 2025ë…„ 8ì›” 11ì¼ 14ì‹œ 00ë¶„
        r"(\d{4})\s*ë…„\s*(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼\s*(\d{1,2})\s*ì‹œ\s*(\d{1,2})\s*ë¶„",
        # 2025ë…„ 8ì›” 11ì¼ 14ì‹œ
        r"(\d{4})\s*ë…„\s*(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼\s*(\d{1,2})\s*ì‹œ",
        # 2025ë…„ 8ì›” 11ì¼
        r"(\d{4})\s*ë…„\s*(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼",
        # 8ì›” 11ì¼ 14ì‹œ 1ë¶„ì˜ 5ì´ˆ (ì˜ ì¡°ì‚¬ í¬í•¨)
        r"(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼\s*(\d{1,2})\s*ì‹œ\s*(\d{1,2})\s*ë¶„ì˜?\s*(\d{1,2})\s*ì´ˆ",
        # 8ì›” 11ì¼ ì˜¤í›„ 2ì‹œ 1ë¶„ 5ì´ˆ (ì˜¤ì „/ì˜¤í›„ í¬í•¨)
        r"(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼\s*(ì˜¤ì „|ì˜¤í›„)\s*(\d{1,2})\s*ì‹œ\s*(\d{1,2})\s*ë¶„\s*(\d{1,2})\s*ì´ˆ",
        # 8ì›” 11ì¼ ì˜¤í›„ 2ì‹œ 1ë¶„ (ì˜¤ì „/ì˜¤í›„ í¬í•¨)
        r"(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼\s*(ì˜¤ì „|ì˜¤í›„)\s*(\d{1,2})\s*ì‹œ\s*(\d{1,2})\s*ë¶„",
        # 8ì›” 11ì¼ ì˜¤í›„ 2ì‹œ (ì˜¤ì „/ì˜¤í›„ í¬í•¨)
        r"(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼\s*(ì˜¤ì „|ì˜¤í›„)\s*(\d{1,2})\s*ì‹œ",
        # 8ì›” 11ì¼ 14ì‹œ 00ë¶„ 05ì´ˆ (ì—°ë„ ì—†ìŒ)
        r"(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼\s*(\d{1,2})\s*ì‹œ\s*(\d{1,2})\s*ë¶„\s*(\d{1,2})\s*ì´ˆ",
        # 8ì›” 11ì¼ 14ì‹œ 00ë¶„ (ì—°ë„ ì—†ìŒ)
        r"(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼\s*(\d{1,2})\s*ì‹œ\s*(\d{1,2})\s*ë¶„",
        # 8ì›” 11ì¼ 14ì‹œ (ì—°ë„ ì—†ìŒ)
        r"(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼\s*(\d{1,2})\s*ì‹œ"
    ]

    for i, pattern in enumerate(korean_patterns):
        m = re.search(pattern, s)
        if m:
            groups = m.groups()
            if i < 4:  # ì—°ë„ê°€ í¬í•¨ëœ íŒ¨í„´
                y, mo, d = int(groups[0]), int(groups[1]), int(groups[2])
                h = int(groups[3]) if len(groups) > 3 and groups[3] else 0
                mi = int(groups[4]) if len(groups) > 4 and groups[4] else 0
                se = int(groups[5]) if len(groups) > 5 and groups[5] else 0
            elif i == 4:  # "8ì›” 11ì¼ 14ì‹œ 1ë¶„ì˜ 5ì´ˆ" íŒ¨í„´
                y = datetime.now().year
                mo, d = int(groups[0]), int(groups[1])
                h = int(groups[2]) if len(groups) > 2 and groups[2] else 0
                mi = int(groups[3]) if len(groups) > 3 and groups[3] else 0
                se = int(groups[4]) if len(groups) > 4 and groups[4] else 0
            elif i >= 5 and i <= 7:  # ì˜¤ì „/ì˜¤í›„ íŒ¨í„´ (5, 6, 7ë²ˆ íŒ¨í„´)
                y = datetime.now().year
                mo, d = int(groups[0]), int(groups[1])
                ampm = groups[2]  # ì˜¤ì „/ì˜¤í›„
                h = int(groups[3]) if len(groups) > 3 and groups[3] else 0
                mi = int(groups[4]) if len(groups) > 4 and groups[4] else 0
                se = int(groups[5]) if len(groups) > 5 and groups[5] else 0

                # ì˜¤ì „/ì˜¤í›„ ì²˜ë¦¬
                if ampm == "ì˜¤í›„" and h != 12:
                    h += 12
                elif ampm == "ì˜¤ì „" and h == 12:
                    h = 0
            else:  # ì—°ë„ê°€ ì—†ëŠ” ì¼ë°˜ íŒ¨í„´ (í˜„ì¬ ì—°ë„ë¡œ ê°€ì •)
                y = datetime.now().year
                mo, d = int(groups[0]), int(groups[1])
                h = int(groups[2]) if len(groups) > 2 and groups[2] else 0
                mi = int(groups[3]) if len(groups) > 3 and groups[3] else 0
                se = int(groups[4]) if len(groups) > 4 and groups[4] else 0

            out.append(f"{y:04d}-{mo:02d}-{d:02d} {h:02d}:{mi:02d}:{se:02d}")
            break  # ì²« ë²ˆì§¸ ë§¤ì¹˜ë§Œ ì‚¬ìš©

    return out

def parse_dt(dt_str: str):
    try:
        s = dt_str.replace("Z", "+00:00")
        dt = datetime.fromisoformat(s)
        return _to_kst_naive(dt)
    except Exception:
        pass
    for f in ["%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M", "%Y-%m-%d %H", "%Y-%m-%d"]:
        try:
            dt = datetime.strptime(dt_str, f)
            return _to_kst_naive(dt)
        except Exception:
            pass
    return None

# === ì§ˆì˜ ë‹¨ìœ„ ê°ì§€ ===
def minute_requested(query: str) -> bool:
    q = query.strip()
    if re.search(r"\d{1,2}\s*ì‹œ\s*\d{1,2}\s*ë¶„", q): return True
    if re.search(r"\b\d{1,2}\s*ë¶„\b", q): return True
    if re.search(r"(?:\b|t)\d{1,2}:\d{2}\b", q, flags=re.IGNORECASE): return True
    return False

def second_requested(query: str) -> bool:
    q = query.strip()
    if re.search(r"\d{1,2}\s*ì´ˆ", q): return True  # \b ì œê±°
    if re.search(r"\b\d{1,2}:\d{2}:\d{2}\b", q): return True
    return False

def hour_bucket_requested(query: str) -> bool:
    q = query.strip()
    if second_requested(q) or minute_requested(q):
        return False
    return bool(re.search(r"(\d{1,2})\s*ì‹œ", q))

def requested_granularity(query: str) -> Optional[str]:
    # ìš°ì„ ìˆœìœ„: ì´ˆ > ë¶„ > ì‹œ
    if second_requested(query): return "second"
    if minute_requested(query) or ("ë¶„ì˜" in query): return "minute"
    if hour_bucket_requested(query): return "hour"
    return None

def get_time_range_from_query(query: str):
    q = query.strip()
    m = re.search(r"(.*?)ë¶€í„°\s+(.*?)ê¹Œì§€", q)
    if m:
        s1, s2 = m.group(1), m.group(2)
        dts = extract_datetime_strings(s1) + extract_datetime_strings(s2)
        if len(dts) >= 2:
            start, end = parse_dt(dts[0]), parse_dt(dts[1])
            if start and end and start < end: return start, end
    m = re.search(r"(.*?)~(.*)", q)
    if m:
        s1, s2 = m.group(1), m.group(2)
        dts = extract_datetime_strings(s1) + extract_datetime_strings(s2)
        if len(dts) >= 2:
            start, end = parse_dt(dts[0]), parse_dt(dts[1])
            if start and end and start < end: return start, end
    m = re.search(r"between\s+(.*?)\s+(?:and|to)\s+(.*)", q, flags=re.I)
    if m:
        s1, s2 = m.group(1), m.group(2)
        dts = extract_datetime_strings(s1) + extract_datetime_strings(s2)
        if len(dts) >= 2:
            start, end = parse_dt(dts[0]), parse_dt(dts[1])
            if start and end and start < end: return start, end
    return None, None

def get_duration_range_from_query(query: str):
    if "ë¶€í„°" not in query: return None, None, None
    start_dt = None
    for ds in extract_datetime_strings(query):
        dt = parse_dt(ds)
        if dt: start_dt = dt; break
    if not start_dt: return None, None, None
    after = query.split("ë¶€í„°", 1)[1]
    m_min = re.search(r"(\d+)\s*ë¶„", after)
    if m_min:
        minutes = int(m_min.group(1))
        if minutes > 0:
            end_dt = start_dt + timedelta(minutes=minutes) - timedelta(seconds=1)
            return start_dt, end_dt, minutes
    m_hr = re.search(r"(\d+)\s*(?:ì‹œê°„|hour|hours)", after, flags=re.I)
    if m_hr:
        hours = int(m_hr.group(1))
        if hours > 0:
            minutes = hours * 60
            end_dt = start_dt + timedelta(hours=hours) - timedelta(seconds=1)
            return start_dt, end_dt, minutes
    m_day = re.search(r"(\d+)\s*(?:ì¼|day|days)", after, flags=re.I)
    if m_day:
        days = int(m_day.group(1))
        if days > 0:
            minutes = days * 24 * 60
            end_dt = start_dt + timedelta(days=days) - timedelta(seconds=1)
            return start_dt, end_dt, minutes
    return None, None, None

def get_minute_to_minute_range(query: str):
    base = None
    for ds in extract_datetime_strings(query):
        dt = parse_dt(ds)
        if dt: base = dt; break
    if not base: return None, None
    m = re.search(r"(\d{1,2})\s*ë¶„ë¶€í„°\s*(\d{1,2})\s*ë¶„ê¹Œì§€", query)
    if m:
        start_min = int(m.group(1)); end_min = int(m.group(2))
        if 0 <= start_min <= 59 and 0 <= end_min <= 59 and end_min > start_min:
            start_dt = base.replace(minute=start_min, second=0)
            end_dt   = base.replace(minute=end_min, second=0) - timedelta(seconds=1)
            return start_dt, end_dt
    m2 = re.search(r"ë¶„ë¶€í„°\s*(\d{1,2})\s*ë¶„ê¹Œì§€", query)
    if m2:
        end_min = int(m2.group(1)); start_min = base.minute
        if 0 <= start_min <= 59 and 0 <= end_min <= 59 and end_min > start_min:
            start_dt = base.replace(minute=start_min, second=0)
            end_dt   = base.replace(minute=end_min, second=0) - timedelta(seconds=1)
            return start_dt, end_dt
    return None, None

# ===== íŒŒì¼ëª…ì—ì„œ ì‹œê°„ ì¶”ì¶œ =====
def parse_time_from_key(key: str):
    """
    íŒŒì¼ëª…/ê²½ë¡œì—ì„œ ì‹œê°„ ë‹¨ì„œë¥¼ ì°¾ì•„ datetime(naive KST)ë¡œ ë°˜í™˜.
    ìš°ì„ ìˆœìœ„: YYYYMMDD_HHMM > YYYYMMDDHH > YYYY-MM-DDTHH:MM > YYYY-MM-DD
    ë°˜í™˜: (dt, granularity)  # granularity in {"minute","hour","day",None}
    """
    base = key.lower()

    # 20250808_1518
    m = re.search(r"(\d{4})(\d{2})(\d{2})[_-]?(\d{2})(\d{2})", base)
    if m:
        y, mo, d, hh, mm = map(int, m.groups())
        # hourtrendë‚˜ houravg ê²½ë¡œë©´ ì‹œê°„ ë‹¨ìœ„ë¡œ ê°•ì œ ì„¤ì •
        if "hourtrend" in base or "houravg" in base:
            return datetime(y, mo, d, hh, 0), "hour"
        return datetime(y, mo, d, hh, mm), "minute"

    # 2025080815 (hour)
    m = re.search(r"(\d{4})(\d{2})(\d{2})(\d{2})(?!\d)", base)
    if m:
        y, mo, d, hh = map(int, m.groups())
        return datetime(y, mo, d, hh, 0), "hour"

    # 2025-08-08T15:18 or 2025-08-08T15
    m = re.search(r"(\d{4})-(\d{2})-(\d{2})t(\d{2})(?::(\d{2}))?", base)
    if m:
        y, mo, d, hh, mm = m.groups()
        y, mo, d, hh = int(y), int(mo), int(d), int(hh)
        mm = int(mm) if mm else 0
        return datetime(y, mo, d, hh, mm), ("minute" if mm else "hour")

    # 2025-08-08 (day)
    m = re.search(r"(\d{4})-(\d{2})-(\d{2})(?![\d:])", base)
    if m:
        y, mo, d = map(int, m.groups())
        return datetime(y, mo, d), "day"

    return None, None

# ===== ìŠ¤ì½”ì–´ë§ =====
def score_doc(query: str, text: str, key: str = "") -> int:
    text_l = text.lower()
    q_tokens = normalize_query_tokens(query)
    score = 0

    # ê¸°ë³¸ íŒŒì¼ íƒ€ì… ì ìˆ˜ (RAG ëª¨ë“œìš©)
    if "rawdata" in key.lower():
        score += 10  # ì›ì‹œ ë°ì´í„°
    elif "minavg" in key.lower() or "mintrend" in key.lower():
        score += 8   # ë¶„ ë‹¨ìœ„ ì§‘ê³„
    elif "hourtrend" in key.lower() or "houravg" in key.lower():
        score += 6   # ì‹œê°„ ë‹¨ìœ„ ì§‘ê³„

    for qt in q_tokens:
        if len(qt) >= 2:
            score += text_l.count(qt)

    # ê¸°ë³¸ í•„ë“œ ì ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼)
    for k in ["\"temperature\"", "\"humidity\"", "\"gas\"", "\"temp\"", "\"hum\""]:
        if k in text_l:
            score += 1

    dt_strs = extract_datetime_strings(query)
    for ds in dt_strs:
        if ds.lower() in text_l:
            score += 5

    # íŒŒì¼ëª…-ì‹œê° ë§¤ì¹­ ê°€ì‚°ì  (ëŒ€í­ ì¦ê°€)
    target_dt = None
    for ds in dt_strs:
        dd = parse_dt(ds)
        if dd:
            target_dt = dd
            break
    if key and target_dt:
        key_dt, gran_key = parse_time_from_key(key)
        if key_dt:
            gran_query = requested_granularity(query)

            # ì •í™•í•œ ì‹œê° ë§¤ì¹­
            if gran_key == "minute" and (key_dt.year,key_dt.month,key_dt.day,key_dt.hour,key_dt.minute) == \
               (target_dt.year,target_dt.month,target_dt.day,target_dt.hour,target_dt.minute):
                score += 100  # ë¶„ ì •í™• ë§¤ì¹­ ì‹œ ëŒ€í­ ê°€ì‚°
            elif gran_key == "hour" and (key_dt.year,key_dt.month,key_dt.day,key_dt.hour) == \
                 (target_dt.year,target_dt.month,target_dt.day,target_dt.hour):
                score += 100  # ì‹œ ì •í™• ë§¤ì¹­ ì‹œ ëŒ€í­ ê°€ì‚°
                if gran_query == "hour":
                    score += 200  # ì‹œê°„ ì§ˆì˜ì™€ ì‹œê°„ íŒŒì¼ ë§¤ì¹­ ì‹œ ì¶”ê°€ ë³´ë„ˆìŠ¤
            elif gran_key == "day" and key_dt.date() == target_dt.date():
                score += 50   # ì¼ ë§¤ì¹­ ì‹œ ê°€ì‚°

    # ì—°ë„ê°€ ëª…ì‹œë˜ê±°ë‚˜ í•œêµ­ì–´ ë‚ ì§œ íŒ¨í„´ì´ ìˆëŠ” ê²½ìš° ë” ì •ë°€í•œ ë°ì´í„° ìš°ì„ ìˆœìœ„ ì ìš©
    has_year = bool(re.search(r'\b(20\d{2})\b', query))
    has_korean_date = bool(re.search(r'\d{1,2}\s*ì›”\s*\d{1,2}\s*ì¼', query))
    requested_gran = requested_granularity(query)

    # ì—°ë„ê°€ ìˆê±°ë‚˜ í•œêµ­ì–´ ë‚ ì§œ íŒ¨í„´ì´ ìˆëŠ” ê²½ìš° ì •ë°€ë„ ìˆœìœ¼ë¡œ ì ìˆ˜ ì¡°ì •
    if has_year or has_korean_date:
        if "\"timestamp\"" in text_l and ("\"temp\"" in text_l or "\"temperature\"" in text_l):
            score += 25  # raw_list ìµœìš°ì„  (ì´ˆ ë‹¨ìœ„ ë°ì´í„°)
        elif "\"averages\"" in text_l and ("\"minute\"" in text_l or "\"calculatedAt\"" in text_l):
            score += 18  # minavg ì°¨ì„  (ë¶„ ë‹¨ìœ„ ë°ì´í„°)
        elif "\"averages\"" in text_l and "\"hourly_ranges\"" in text_l:
            score += 8   # houravg ìµœí•˜ìœ„ (ì‹œê°„ ë‹¨ìœ„ ë°ì´í„°)

    elif requested_gran == "second":
        # ì´ˆ ë‹¨ìœ„ ìš”ì²­: raw_listë¥¼ ìµœìš°ì„ 
        if "\"timestamp\"" in text_l and ("\"temp\"" in text_l or "\"temperature\"" in text_l):
            score += 35  # raw_list ëŒ€í­ ìš°ëŒ€
        elif "rawdata" in key.lower():
            score += 30  # rawdata ê²½ë¡œ ëŒ€í­ ìš°ëŒ€
        if "\"averages\"" in text_l:
            score -= 10  # ì§‘ê³„ ë°ì´í„° ëŒ€í­ ê°ì 
        if "\"hourly_ranges\"" in text_l:
            score -= 15  # houravg ëŒ€í­ ê°ì 

    elif requested_gran == "minute":
        # ë¶„ ë‹¨ìœ„ ìš”ì²­: minavgë¥¼ ìµœìš°ì„ , ê·¸ë‹¤ìŒ raw_list
        if "\"averages\"" in text_l and ("\"minute\"" in text_l or "\"timestamp\"" in text_l or "\"calculatedAt\"" in text_l):
            score += 30  # minavg ëŒ€í­ ìš°ëŒ€
        elif "minavg" in key.lower() or "mintrend" in key.lower():
            score += 25  # minavg ê²½ë¡œ ëŒ€í­ ìš°ëŒ€
        if "\"timestamp\"" in text_l and ("\"temp\"" in text_l or "\"temperature\"" in text_l):
            score += 15  # raw_listë„ ìš°ëŒ€ (í•˜ì§€ë§Œ minavgë³´ë‹¤ ë‚®ìŒ)
        if "\"hourly_ranges\"" in text_l:
            score -= 10  # houravg ê°ì 

    elif requested_gran == "hour":
        # ì‹œ ë‹¨ìœ„ ìš”ì²­: houravgë¥¼ ëŒ€í­ ìš°ì„ 
        hour_bonus_applied = False
        if "\"averages\"" in text_l and "\"hourly_ranges\"" in text_l:
            score += 50  # houravg ëŒ€í­ ìš°ëŒ€
            hour_bonus_applied = True
        elif "\"hourtemp\"" in text_l or "\"hourhum\"" in text_l or "\"hourgas\"" in text_l:
            score += 50  # ì‹œê°„ë‹¨ìœ„ í•„ë“œê°€ ìˆëŠ” íŒŒì¼ ëŒ€í­ ìš°ëŒ€
            hour_bonus_applied = True
        elif "hourtrend" in key.lower() or "houravg" in key.lower():
            score += 45  # íŒŒì¼ê²½ë¡œì— hourtrend/houravgê°€ ìˆìœ¼ë©´ ëŒ€í­ ìš°ëŒ€
            hour_bonus_applied = True

        # ì‹œê°„ ë‹¨ìœ„ ìš”ì²­ì—ì„œëŠ” raw dataì— í˜ë„í‹° ë¶€ì—¬
        if "\"timestamp\"" in text_l and ("\"temp\"" in text_l or "\"temperature\"" in text_l):
            if not hour_bonus_applied:  # houravg ë³´ë„ˆìŠ¤ê°€ ì—†ëŠ” ê²½ìš°ì—ë§Œ ì•½ê°„ ìš°ëŒ€
                score += 5
            else:
                score -= 10  # houravg íŒŒì¼ì´ ìˆëŠ” ê²½ìš° raw dataì— í˜ë„í‹°
        if "\"averages\"" in text_l and "\"minute\"" in text_l:
            score -= 10   # minavg ëŒ€í­ ê°ì 

    else:
        # ë‹¨ìœ„ê°€ ëª…ì‹œë˜ì§€ ì•Šì€ ì¼ë°˜ ì§ˆì˜: ê· í˜•ìˆê²Œ
        if "\"timestamp\"" in text_l and ("\"temp\"" in text_l or "\"temperature\"" in text_l):
            score += 8   # raw_list ìš°ëŒ€
        if "\"averages\"" in text_l and ("\"minute\"" in text_l or "\"timestamp\"" in text_l):
            score += 5   # minavg ì¤‘ê°„
        if "\"averages\"" in text_l and "\"hourly_ranges\"" in text_l:
            score += 3   # houravg ë‚®ìŒ

    # ë””ë²„ê¹… ì½”ë“œ ì œê±° (is_debug ë³€ìˆ˜ ë¬¸ì œë¡œ ì¸í•´)

    return score

# ===== JSON ìŠ¤í‚¤ë§ˆ ê°ì§€ =====
def detect_schema(obj):
    """
    returns: "raw_list" | "minavg" | "houravg" | "mintrend" | None
    """
    # rawdata: ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ 5ì´ˆ ê°„ê²© ë°ì´í„°
    if isinstance(obj, list) and obj and isinstance(obj[0], dict):
        k = set(obj[0].keys())
        if {"timestamp", "temp", "hum", "gas"}.issubset(k): return "raw_list"
        if {"timestamp", "temperature", "humidity", "gas"}.issubset(k): return "raw_list"

    if isinstance(obj, dict):
        # hourtrend: averagesì™€ hourly_ranges, trendsë¥¼ ëª¨ë‘ ê°€ì§„ êµ¬ì¡°
        if "averages" in obj and "hourly_ranges" in obj and "trends" in obj:
            return "houravg"

        # houravg: ë‹¨ìˆœí•œ ì‹œê°„ë³„ í‰ê·  (hourtemp, hourhum, hourgas)
        if "hourtemp" in obj and "hourhum" in obj and "hourgas" in obj:
            return "houravg"

        # minavg: ë¶„ë³„ í‰ê·  (mintemp, minhum, mingas)
        if "mintemp" in obj and "minhum" in obj and "mingas" in obj:
            return "minavg"

        # mintrend: data ì•ˆì— ë¶„ë³„ ë°ì´í„°ê°€ ìˆëŠ” êµ¬ì¡°
        if "data" in obj and isinstance(obj["data"], dict):
            data = obj["data"]
            if "mintemp" in data and "minhum" in data and "mingas" in data:
                return "mintrend"

        # ê¸°ì¡´ averages ê¸°ë°˜ ê°ì§€ (ë°±ì—…ìš©)
        if "averages" in obj and ("minute" in obj or "timestamp" in obj or "calculatedAt" in obj):
            return "minavg"

    return None

# ===== S3 ë‹¤ìš´ë¡œë“œ/ìŠ¤ì½”ì–´ (ìŠ¤í‚¤ë§ˆ í¬í•¨) =====
def download_and_score_file(key: str, query: str):
    try:
        head_resp = s3.head_object(Bucket=S3_BUCKET_DATA, Key=key)
        file_size = head_resp.get('ContentLength', 0)
        if file_size > MAX_FILE_SIZE:
            obj = s3.get_object(Bucket=S3_BUCKET_DATA, Key=key, Range=f"bytes=0-{MAX_FILE_SIZE-1}")
        else:
            obj = s3.get_object(Bucket=S3_BUCKET_DATA, Key=key)
        data = obj["Body"].read()
        txt = data.decode("utf-8", errors="ignore")
        if not txt.strip():
            return None

        schema = None
        j = None
        try:
            # ë¨¼ì € ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ JSONìœ¼ë¡œ íŒŒì‹± ì‹œë„
            j = json.loads(txt)
            schema = detect_schema(j)
        except Exception:
            try:
                # ì‹¤íŒ¨í•˜ë©´ JSONì´ ì—¬ëŸ¬ ì¤„ë¡œ ë˜ì–´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¼ì¸ë³„ë¡œ íŒŒì‹±
                lines = txt.strip().split('\n')
                if len(lines) == 1:
                    # í•œ ì¤„ì´ë©´ ë‹¨ì¼ ê°ì²´
                    j = json.loads(lines[0])
                    schema = detect_schema(j)
                else:
                    # ì—¬ëŸ¬ ì¤„ì´ë©´ JSON Lines í˜•íƒœì¼ ê°€ëŠ¥ì„±
                    json_objects = []
                    for line in lines:
                        line = line.strip()
                        if line:
                            json_objects.append(json.loads(line))
                    if json_objects:
                        if len(json_objects) == 1:
                            j = json_objects[0]
                        else:
                            j = json_objects  # ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
                        schema = detect_schema(j)
            except Exception:
                # ë§ˆì§€ë§‰ìœ¼ë¡œ ê¸°ì¡´ ë°©ì‹ ì‹œë„
                try:
                    start = txt.find("{"); alt_start = txt.find("[")
                    if alt_start != -1 and (start == -1 or alt_start < start): start = alt_start
                    end = max(txt.rfind("}"), txt.rfind("]"))
                    if start != -1 and end != -1 and end > start:
                        j = json.loads(txt[start:end+1])
                        schema = detect_schema(j)
                except Exception:
                    pass

        sc = score_doc(query, txt, key=key)

        # ê°„ë‹¨í•œ ìŠ¤í‚¤ë§ˆ ì ìˆ˜ (RAG ëª¨ë“œìš©)
        if schema == "raw_list": sc += 5
        elif schema == "minavg": sc += 4
        elif schema == "houravg": sc += 3

        return {"id": key, "content": txt, "score": sc, "file_size": file_size, "schema": schema, "json": j}
    except Exception:
        return None

# ===== ë¹ ë¥¸ ì¦ê±° ìŠ¤ë‹ˆí•‘ =====
def quick_sensor_evidence(query: str, max_probe: int = 6) -> dict:
    paginator = s3.get_paginator("list_objects_v2")
    pages = paginator.paginate(Bucket=S3_BUCKET_DATA, Prefix=S3_PREFIX)

    keys = []
    for page in pages:
        for obj in page.get("Contents", []):
            k = obj["Key"]
            if not k.lower().endswith(".json"):
                continue
            keys.append(k)
            if len(keys) >= MAX_FILES_TO_SCAN:
                break
        if len(keys) >= MAX_FILES_TO_SCAN:
            break
    if not keys:
        return {'has_schema': False, 'best_schema': None, 'best_score': 0}

    scored = []
    with _f.ThreadPoolExecutor(max_workers=min(6, MAX_WORKERS)) as ex:
        futs = {ex.submit(download_and_score_file, k, query): k for k in keys[:max_probe]}
        for f in _f.as_completed(futs):
            r = f.result()
            if r:
                scored.append(r)
    if not scored:
        return {'has_schema': False, 'best_schema': None, 'best_score': 0}

    top = sorted(scored, key=lambda x: x["score"], reverse=True)[0]
    return {
        'has_schema': top.get("schema") in {"raw_list","minavg","houravg"},
        'best_schema': top.get("schema"),
        'best_score': top.get("score", 0)
    }

# ===== LLM ê¸°ë°˜ ì˜ë„ ë¶„ë¥˜ =====
def _build_intent_prompt(query: str) -> str:
    return (
        "You are a router. Classify the user's query domain.\n"
        "Return STRICT JSON: {\"domain\": \"sensor_data\"|\"general\", \"confidence\": 0.0-1.0}.\n"
        "- Choose \"sensor_data\" ONLY if the user is asking about IoT environmental readings "
        "(temperature/humidity/gas/ppm) from my stored device data, with a time window "
        "(íŠ¹ì • ë‚ ì§œ/ì‹œ/ë¶„/ì´ˆ, 'ë¶€í„°~ê¹Œì§€', 'ìµœê·¼', 'ì²˜ìŒ/ë§ˆì§€ë§‰') or stats (í‰ê· /ìµœëŒ€/ìµœì†Œ/ì¶”ì´ ë“±).\n"
        "- Weather forecasts, sports, finance, ì¼ë°˜ ìƒì‹ ë“±ì€ \"general\".\n"
        "- IMPORTANT: í•œêµ­ì–´ ì§ˆì˜ì—ì„œ 'ë‚ ì”¨ ì˜ˆë³´'ê°€ ì•„ë‹ˆë¼ 'ë‚´ ì„¼ì„œ ë¡œê·¸'ì¼ ìˆ˜ ìˆìŒ.\n\n"
        "Examples:\n"
        "Q: ë©”ì‹œì˜ ê²½ê¸°ë§ˆë‹¤ í‰ê·  ëª‡ ê³¨ì„ ë„£ì–´? â†’ {\"domain\":\"general\",\"confidence\":0.95}\n"
        "Q: ë‚´ì¼ ì„œìš¸ ë‚ ì”¨ ì–´ë•Œ? â†’ {\"domain\":\"general\",\"confidence\":0.95}\n"
        "Q: 2025ë…„ 8ì›” 8ì¼ 16ì‹œ ì˜¨ë„, ìŠµë„, ê³µê¸°ì§ˆì„ ì•Œë ¤ì¤˜ â†’ {\"domain\":\"sensor_data\",\"confidence\":0.95}\n"
        "Q: 2025-08-11 10:15:15 ìŠµë„? â†’ {\"domain\":\"sensor_data\",\"confidence\":0.95}\n"
        "Q: ìµœê·¼ ê³µê¸°ì§ˆ í‰ê·  ë³´ì—¬ì¤˜ â†’ {\"domain\":\"sensor_data\",\"confidence\":0.9}\n"
        "Q: esp32s3-airwatch 15:18 ì˜¨ë„ í‰ê·  â†’ {\"domain\":\"sensor_data\",\"confidence\":0.95}\n\n"
        f"query: {query}\n"
        "json:"
    )

def _invoke_claude(messages, max_tokens=512, temperature=0.0, top_p=0.9, system=None):
    body = {
        "anthropic_version": "bedrock-2023-05-31",
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": top_p,
    }
    if system:
        body["system"] = system

    resp = bedrock_rt.invoke_model(
        modelId=INFERENCE_PROFILE_ARN,
        accept="application/json",
        contentType="application/json",
        body=json.dumps(body).encode("utf-8"),
    )
    payload = json.loads(resp["body"].read().decode("utf-8", errors="ignore"))
    text = "".join(
        p.get("text", "")
        for p in (payload.get("content") or [])
        if isinstance(p, dict) and p.get("type") == "text"
    ).strip()
    return text, payload

@lru_cache(maxsize=256)
def classify_query_with_llm(query: str) -> dict:
    user_text = _build_intent_prompt(query)
    messages = [
        {"role": "user", "content": [{"type": "text", "text": user_text}]}
    ]
    text, _raw = _invoke_claude(messages, max_tokens=64, temperature=0.0, top_p=0.9)
    try:
        out = json.loads(text)
        dom = out.get("domain", "general")
        conf = float(out.get("confidence", 0.0))
        if dom not in ("sensor_data","general"): dom = "general"
        conf = max(0.0, min(1.0, conf))
        return {"domain": dom, "confidence": conf}
    except Exception:
        return {"domain": "general", "confidence": 0.0}

# ===== ê²°ì •ì  ì‹ í˜¸(ì„¼ì„œ ë‹¨ì–´ + ì‹œê°„/êµ¬ê°„ í† í°) ê°€ë“œë ˆì¼ =====
_TIME_HINTS = ("ë…„", "ì›”", "ì¼", "ì‹œ", "ë¶„", "ì´ˆ", "-", ":", "ë¶€í„°", "ê¹Œì§€", "~", "between")
_RANGE_HINTS = ("êµ¬ê°„", "ìµœê·¼", "ì²˜ìŒ", "ì²«", "ë§ˆì§€ë§‰", "ìµœì¢…")
def _deterministic_sensor_signal(query: str) -> bool:
    fields = detect_fields_in_query(query)
    if not fields:
        return False
    has_time_literal = bool(extract_datetime_strings(query))
    has_ko_time_tokens = any(tok in query for tok in _TIME_HINTS)
    has_range = any(tok in query for tok in _RANGE_HINTS)
    return has_time_literal or has_ko_time_tokens or has_range

def decide_route(query: str) -> str:
    if _deterministic_sensor_signal(query):
        return "sensor"

    cls = classify_query_with_llm(query)
    dom, conf = cls["domain"], cls["confidence"]

    if dom == "sensor_data" and conf >= 0.6:
        return "sensor"
    if 0.4 <= conf < 0.6:
        ev = quick_sensor_evidence(query)
        if ev["has_schema"] and ev["best_score"] >= RELEVANCE_THRESHOLD:
            return "sensor"
        return "general"
    return "general"

# ===== ê²€ìƒ‰ =====
def retrieve_documents_from_s3(query: str, limit_chars: int = LIMIT_CONTEXT_CHARS, max_files: int = MAX_FILES_TO_SCAN, top_k: int = TOP_K):
    # ë‚ ì§œë³„ prefix í•„í„°ë§ìœ¼ë¡œ ê²€ìƒ‰ ìµœì í™”
    dt_strings = extract_datetime_strings(query)
    target_dt = None
    date_prefixes = []

    # ì¿¼ë¦¬ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
    for ds in dt_strings:
        dt = parse_dt(ds)
        if dt:
            target_dt = dt
            date_prefix = dt.strftime('%Y%m%d')  # YYYYMMDD í˜•ì‹
            date_prefixes.append(date_prefix)
            break

    gran = requested_granularity(query)
    paginator = s3.get_paginator("list_objects_v2")
    priority_keys = []

    # ë‚ ì§œê°€ ëª…ì‹œëœ ê²½ìš° í•´ë‹¹ ë‚ ì§œ í´ë”ë§Œ ê²€ìƒ‰
    if date_prefixes:
        date_prefix = date_prefixes[0]

        if gran == "hour":
            # ì‹œê°„ ì§ˆì˜: í•´ë‹¹ ë‚ ì§œì˜ hourtrend/houravgë§Œ ê²€ìƒ‰
            for prefix_path in ["hourtrend/", "houravg/"]:
                try:
                    search_prefix = f"{S3_PREFIX}{prefix_path}{date_prefix}/"
                    pages = paginator.paginate(Bucket=S3_BUCKET_DATA, Prefix=search_prefix, PaginationConfig={'MaxItems': 50})
                    for page in pages:
                        for obj in page.get("Contents", []):
                            k = obj["Key"]
                            if k.lower().endswith(".json"):
                                priority_keys.append(k)
                            if len(priority_keys) >= 30:
                                break
                        if len(priority_keys) >= 30:
                            break
                except Exception:
                    pass
        elif gran == "minute":
            # ë¶„ ì§ˆì˜: í•´ë‹¹ ë‚ ì§œì˜ minavg/mintrendë§Œ ê²€ìƒ‰
            for prefix_path in ["minavg/", "mintrend/"]:
                try:
                    search_prefix = f"{S3_PREFIX}{prefix_path}{date_prefix}/"
                    pages = paginator.paginate(Bucket=S3_BUCKET_DATA, Prefix=search_prefix, PaginationConfig={'MaxItems': 50})
                    for page in pages:
                        for obj in page.get("Contents", []):
                            k = obj["Key"]
                            if k.lower().endswith(".json"):
                                priority_keys.append(k)
                            if len(priority_keys) >= 30:
                                break
                        if len(priority_keys) >= 30:
                            break
                except Exception:
                    pass

        # í•´ë‹¹ ë‚ ì§œì˜ rawdataë„ ê²€ìƒ‰
        try:
            search_prefix = f"{S3_PREFIX}rawdata/{date_prefix}/"
            pages = paginator.paginate(Bucket=S3_BUCKET_DATA, Prefix=search_prefix, PaginationConfig={'MaxItems': 100})
            for page in pages:
                for obj in page.get("Contents", []):
                    k = obj["Key"]
                    if k.lower().endswith(".json"):
                        priority_keys.append(k)
                    if len(priority_keys) >= 100:
                        break
                if len(priority_keys) >= 100:
                    break
        except Exception:
            pass

        # ë‚ ì§œë³„ ê²€ìƒ‰ìœ¼ë¡œ ì¶©ë¶„í•œ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì „ì²´ ê²€ìƒ‰ ìƒëµ
        if len(priority_keys) >= 50:
            keys = priority_keys[:max_files]
        else:
            # ì¶”ê°€ ê²€ìƒ‰ í•„ìš”ì‹œë§Œ ì œí•œì  ì „ì²´ ê²€ìƒ‰
            keys = priority_keys
            pages = paginator.paginate(Bucket=S3_BUCKET_DATA, Prefix=S3_PREFIX, PaginationConfig={'MaxItems': max_files//2})
    else:
        # ë‚ ì§œê°€ ëª…ì‹œë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ì¡´ ë°©ì‹
        if gran == "hour":
            for prefix_path in ["hourtrend/", "houravg/"]:
                try:
                    search_prefix = S3_PREFIX + prefix_path
                    pages = paginator.paginate(Bucket=S3_BUCKET_DATA, Prefix=search_prefix, PaginationConfig={'MaxItems': 50})
                    for page in pages:
                        for obj in page.get("Contents", []):
                            k = obj["Key"]
                            if k.lower().endswith(".json"):
                                priority_keys.append(k)
                            if len(priority_keys) >= 50:
                                break
                        if len(priority_keys) >= 50:
                            break
                except Exception:
                    pass
        elif gran == "minute":
            for prefix_path in ["minavg/", "mintrend/"]:
                try:
                    pages = paginator.paginate(Bucket=S3_BUCKET_DATA, Prefix=S3_PREFIX + prefix_path, PaginationConfig={'MaxItems': 50})
                    for page in pages:
                        for obj in page.get("Contents", []):
                            k = obj["Key"]
                            if k.lower().endswith(".json"):
                                priority_keys.append(k)
                            if len(priority_keys) >= 50:
                                break
                        if len(priority_keys) >= 50:
                            break
                except Exception:
                    pass

        keys = priority_keys
        # ì œí•œì  ì „ì²´ ê²€ìƒ‰
        pages = paginator.paginate(Bucket=S3_BUCKET_DATA, Prefix=S3_PREFIX, PaginationConfig={'MaxItems': max_files})
    for page in pages:
        for obj in page.get("Contents", []):
            k = obj["Key"]
            if not k.lower().endswith(".json"):
                continue
            keys.append(k)
            if len(keys) >= max_files:
                break
        if len(keys) >= max_files:
            break

    # ìš°ì„  í‚¤ë“¤ì„ ì•ì— ë°°ì¹˜
    all_keys = list(set(priority_keys + keys))[:max_files]

    if not all_keys: return [], ""

    scored = []
    with _f.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_key = {executor.submit(download_and_score_file, key, query): key for key in all_keys}
        for future in _f.as_completed(future_to_key):
            result = future.result()
            if result: scored.append(result)

    if not scored: return [], ""

    top = sorted(scored, key=lambda x: x["score"], reverse=True)[:top_k]

    # ì»¨í…ìŠ¤íŠ¸(LLM ë°±ì—…ìš©)
    parts, context_length = [], 0
    for idx, d in enumerate(top, start=1):
        tag = f"D{idx}"; d["tag"] = tag
        remaining_space = limit_chars - context_length - 200
        if remaining_space <= 0: break
        content = d["content"]
        if len(content) > remaining_space:
            content = content[:remaining_space] + "\n[ë¬¸ì„œê°€ ê¸¸ì–´ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤...]"
        part = f"[{tag}] (s3://{S3_BUCKET_DATA}/{d['id']})\n{content}\n"
        parts.append(part)
        context_length += len(part)

    context = "\n---\n".join(parts).strip()
    return top, context

# ===== í†µê³„/ì¶”ì´/ìœˆë„ìš° ìœ í‹¸ =====
def select_rows_in_range(rows, start_dt, end_dt):
    return [r for r in rows if start_dt <= r["timestamp"] <= end_dt]

def select_rows_in_minute(rows, dt_minute: datetime):
    m_start = dt_minute.replace(second=0)
    m_end = m_start + timedelta(minutes=1) - timedelta(seconds=1)
    return [r for r in rows if m_start <= r["timestamp"] <= m_end], m_start, m_end

def select_rows_in_hour(rows, dt_hour: datetime):
    h_start = dt_hour.replace(minute=0, second=0)
    h_end = h_start + timedelta(hours=1) - timedelta(seconds=1)
    return [r for r in rows if h_start <= r["timestamp"] <= h_end], h_start, h_end

def select_rows_in_day(rows, dt_day: datetime):
    d_start = dt_day.replace(hour=0, minute=0, second=0)
    d_end = d_start + timedelta(days=1) - timedelta(seconds=1)
    return [r for r in rows if d_start <= r["timestamp"] <= d_end], d_start, d_end

def compute_stats(rows):
    if not rows: return None
    keys = set().union(*[set(r.keys()) for r in rows]) - {"timestamp"}
    out = {}
    for k in ["temperature","humidity","gas"]:
        if k in keys:
            arr = [r[k] for r in rows if k in r]
            if arr:
                out[k] = {"avg": sum(arr)/len(arr), "min": min(arr), "max": max(arr), "first": arr[0], "last": arr[-1]}
    return out

def compare_trend(curr_stat, prev_stat):
    def diff_pct(a, b):
        if b is None or a is None or b == 0: return None
        return (a - b) / b * 100.0
    out = {}
    for field in ["temperature", "humidity", "gas"]:
        cs = curr_stat.get(field) if curr_stat else None
        ps = prev_stat.get(field) if prev_stat else None
        if not cs or not ps: out[field] = None; continue
        base_curr = cs.get("avg") if cs.get("avg") is not None else cs.get("last")
        base_prev = ps.get("avg") if ps.get("avg") is not None else ps.get("last")
        if base_curr is None or base_prev is None:
            out[field] = None; continue
        delta = base_curr - base_prev
        pct = diff_pct(base_curr, base_prev)
        out[field] = {"delta": delta, "pct": pct}
    return out

def fmt_trend_line(field_kor, stat, trend):
    if not stat: return f"{field_kor}: ë°ì´í„° ì—†ìŒ"
    avg_s = f"í‰ê·  {stat['avg']:.3f}, ë²”ìœ„ [{stat['min']:.3f}~{stat['max']:.3f}]"
    if not trend: return f"{field_kor}: {avg_s}"
    delta = trend["delta"]; pct = trend["pct"]
    if delta is None: return f"{field_kor}: {avg_s}"
    dir_word = "ì¦ê°€" if delta > 0 else ("ê°ì†Œ" if delta < 0 else "ë³€í™” ì—†ìŒ")
    pct_s = f"{pct:+.2f}%" if pct is not None else "N/A"
    return f"{field_kor}: {avg_s} | ì§ì „ êµ¬ê°„ ëŒ€ë¹„ {dir_word} ({delta:+.3f}, {pct_s})"

def filter_fields(row: dict, need_fields: set):
    if not need_fields:
        return {k: row[k] for k in ["temperature","humidity","gas"] if k in row}
    return {f: row[f] for f in need_fields if f in row}

def format_point_answer(values: dict, ts: datetime, tag="D1"):
    parts = []
    for k in ["temperature", "humidity", "gas"]:
        if k in values:
            name = FIELD_NAME_KOR.get(k, k)
            value = values[k]
            comment = get_friendly_comment(k, value)
            parts.append(f"{name} **{value}** {comment}")

    if not parts:
        body = "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    elif len(parts) == 1:
        body = f"í•´ë‹¹ ì‹œì ì˜ {parts[0]}"
    else:
        body = f"ğŸ“Š **ì •í™•í•œ ì‹œì  ë°ì´í„°**\n" + "\n".join([f"â€¢ {part}" for part in parts])

    return f"{ts.strftime('%Y-%m-%d %H:%M:%S')} ê¸°ì¤€:\n{body} [{tag}]"

def format_window_answer(rows_in_window, w_start, w_end, need_fields, tag="D1", window_name="êµ¬ê°„", show_samples=True):
    fields = list(need_fields) if need_fields else [k for k in ["temperature","humidity","gas"] if any(k in r for r in rows_in_window)]
    name_map = FIELD_NAME_KOR
    lines = [f"[{window_name}] {w_start.strftime('%Y-%m-%d %H:%M:%S')} ~ {w_end.strftime('%Y-%m-%d %H:%M:%S')}"]
    for f in fields:
        arr = [r[f] for r in rows_in_window if f in r]
        if arr:
            a = sum(arr)/len(arr)
            lines.append(f"{name_map.get(f,f)} í‰ê· : {a:.3f}")
        else:
            lines.append(f"{name_map.get(f,f)} í‰ê· : ë°ì´í„° ì—†ìŒ")
    if show_samples:
        lines.append(f"[{window_name} ë°ì´í„° {len(rows_in_window)}ê°œ]")
        for r in rows_in_window:
            parts = []
            if "temperature" in fields and "temperature" in r: parts.append(f"T={r['temperature']}")
            if "humidity" in fields and "humidity" in r:    parts.append(f"H={r['humidity']}")
            if "gas" in fields and "gas" in r:               parts.append(f"CO2={r['gas']}")
            lines.append(f"{r['timestamp'].strftime('%Y-%m-%d %H:%M:%S')} | " + ", ".join(parts))
    else:
        lines.append(f"(ìƒ˜í”Œ {len(rows_in_window)}ê°œëŠ” ìƒëµë¨ â€” 'ìƒì„¸' ë˜ëŠ” 'ì›ë³¸'ì´ë¼ê³  ë¬¼ìœ¼ë©´ ì „ë¶€ ë³´ì—¬ì¤„ê²Œ)")
    return "\n".join(lines) + f" [{tag}]"

# ===== RAW ë³€í™˜ =====
def _load_raw_rows(j):
    rows = []
    # rawdata: ë¦¬ìŠ¤íŠ¸ í˜•íƒœ
    if isinstance(j, list):
        for i, r in enumerate(j):
            try:
                ts = parse_dt(str(r["timestamp"]))
                if not ts: continue
                temperature = float(r["temperature"]) if "temperature" in r else float(r["temp"])
                humidity    = float(r["humidity"]) if "humidity" in r else float(r["hum"])
                gas         = float(r["gas"])
                rows.append({"timestamp": ts, "temperature": temperature, "humidity": humidity, "gas": gas})
            except Exception:
                continue
    # ë‹¨ì¼ í•­ëª© ë°ì´í„°ë“¤ì„ í–‰ìœ¼ë¡œ ë³€í™˜
    elif isinstance(j, dict):
        try:
            # houravg í˜•íƒœ (hourtemp, hourhum, hourgas)
            if "hourtemp" in j:
                ts = parse_dt(str(j["timestamp"]))
                if ts:
                    rows.append({
                        "timestamp": ts,
                        "temperature": float(j["hourtemp"]),
                        "humidity": float(j["hourhum"]),
                        "gas": float(j["hourgas"])
                    })
            # minavg í˜•íƒœ (mintemp, minhum, mingas)
            elif "mintemp" in j:
                ts = parse_dt(str(j["timestamp"]))
                if ts:
                    rows.append({
                        "timestamp": ts,
                        "temperature": float(j["mintemp"]),
                        "humidity": float(j["minhum"]),
                        "gas": float(j["mingas"])
                    })
            # mintrend í˜•íƒœ (data ì•ˆì— ìˆìŒ)
            elif "data" in j and "mintemp" in j["data"]:
                data = j["data"]
                ts = parse_dt(str(data["timestamp"]))
                if ts:
                    rows.append({
                        "timestamp": ts,
                        "temperature": float(data["mintemp"]),
                        "humidity": float(data["minhum"]),
                        "gas": float(data["mingas"])
                    })
        except Exception:
            pass

    rows.sort(key=lambda x: x["timestamp"])
    return rows

# ====== ë§ˆì§€ë§‰ ì„¼ì„œ ì§ˆì˜ ì»¨í…ìŠ¤íŠ¸ ======
LAST_SENSOR_CTX: Dict[str, object] = {
    "window": None,  # "second" | "minute" | "hour" | "range" | None
    "start": None,   # datetime
    "end": None,     # datetime
    "rows": None,    # List[dict] (RAW rows)
    "tag": None,     # "D1" ë“±
    "label": None    # "í•´ë‹¹ ë¶„" ë“±
}

def _reset_last_ctx():
    LAST_SENSOR_CTX.update({"window": None, "start": None, "end": None, "rows": None, "tag": None, "label": None})

def _set_last_ctx(window: str, start: datetime, end: datetime, rows: List[dict], tag: str, label: str):
    LAST_SENSOR_CTX["window"] = window
    LAST_SENSOR_CTX["start"] = start
    LAST_SENSOR_CTX["end"] = end
    LAST_SENSOR_CTX["rows"] = rows
    LAST_SENSOR_CTX["tag"] = tag
    LAST_SENSOR_CTX["label"] = label

def _format_full_rows(rows: List[dict], start: datetime, end: datetime, tag: str, label: str) -> str:
    lines = [f"[{label} ìƒì„¸] {start.strftime('%Y-%m-%d %H:%M:%S')} ~ {end.strftime('%Y-%m-%d %H:%M:%S')} | ìƒ˜í”Œ {len(rows)}ê°œ"]
    for r in rows:
        t = r["timestamp"].strftime("%Y-%m-%d %H:%M:%S")
        parts = []
        if "temperature" in r: parts.append(f"T={r['temperature']}")
        if "humidity" in r:    parts.append(f"H={r['humidity']}")
        if "gas" in r:         parts.append(f"CO2={r['gas']}")
        lines.append(f"{t} | " + ", ".join(parts))
    return "\n".join(lines) + f" [{tag}]"

# ---- RAW ì „ì²´ ì¬ìˆ˜ì§‘/ì •í™• ë§¤ì¹­ ----
def fetch_raw_rows_for_window_all(start: datetime, end: datetime, max_files: int = MAX_FILES_TO_SCAN) -> Tuple[List[dict], Optional[str]]:
    paginator = s3.get_paginator("list_objects_v2")
    pages = paginator.paginate(Bucket=S3_BUCKET_DATA, Prefix=S3_PREFIX)

    keys = []
    for page in pages:
        for obj in page.get("Contents", []):
            k = obj["Key"]
            if not k.lower().endswith(".json"):
                continue
            keys.append(k)
            if len(keys) >= max_files:
                break
        if len(keys) >= max_files:
            break

    all_rows = []
    raw_tag = None
    with _f.ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
        futs = {ex.submit(download_and_score_file, k, f"{start}~{end}"): k for k in keys}
        for f in _f.as_completed(futs):
            r = f.result()
            if not r:
                continue

            # ëª¨ë“  ë°ì´í„° íƒ€ì…ì„ í—ˆìš©
            schema = r.get("schema")
            file_path = r.get("id", "").lower()

            if schema not in ["raw_list", "houravg", "minavg", "mintrend", None]:
                continue

            # rawdata, houravg, minavg, mintrend íŒŒì¼ë“¤ì€ ëª¨ë‘ ì²˜ë¦¬ ëŒ€ìƒ
            if not any(pattern in file_path for pattern in ["rawdata", "houravg", "minavg", "mintrend"]) and schema is None:
                continue
            rows = _load_raw_rows(r.get("json") or [])
            if not rows:
                continue
            subset = select_rows_in_range(rows, start, end)
            if subset:
                all_rows.extend(subset)
                if raw_tag is None:
                    raw_tag = "D?"
    all_rows.sort(key=lambda x: x["timestamp"])
    return all_rows, raw_tag

def fetch_raw_exact_second_all(target_dt: datetime, max_files: int = MAX_FILES_TO_SCAN) -> Tuple[Optional[dict], Optional[str]]:
    paginator = s3.get_paginator("list_objects_v2")
    pages = paginator.paginate(Bucket=S3_BUCKET_DATA, Prefix=S3_PREFIX)
    with _f.ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
        futures = []
        scanned = 0
        for page in pages:
            for obj in page.get("Contents", []):
                k = obj["Key"]
                if not k.lower().endswith(".json"):
                    continue
                futures.append(ex.submit(download_and_score_file, k, str(target_dt)))
                scanned += 1
                if scanned >= max_files:
                    break
            if scanned >= max_files:
                break
        for f in _f.as_completed(futures):
            r = f.result()
            if not r or r.get("schema") != "raw_list":
                continue
            rows = _load_raw_rows(r.get("json") or [])
            for row in rows:
                if row["timestamp"] == target_dt:
                    return row, "D?"
    return None, None

def show_last_detail_if_any(query: str) -> Optional[str]:
    if not want_detail_list(query):
        return None
    if not LAST_SENSOR_CTX.get("start") or not LAST_SENSOR_CTX.get("end"):
        return None
    if not LAST_SENSOR_CTX.get("rows"):
        rows, raw_tag = fetch_raw_rows_for_window_all(LAST_SENSOR_CTX["start"], LAST_SENSOR_CTX["end"])
        if rows:
            _set_last_ctx(
                window=LAST_SENSOR_CTX.get("window") or "range",
                start=LAST_SENSOR_CTX["start"],
                end=LAST_SENSOR_CTX["end"],
                rows=rows,
                tag=raw_tag or LAST_SENSOR_CTX.get("tag") or "D?",
                label=LAST_SENSOR_CTX.get("label") or "ìš”ì²­ êµ¬ê°„",
            )
        else:
            return "(ìµœê·¼ ì„¼ì„œ êµ¬ê°„ì˜ ì›ë³¸ ìƒ˜í”Œì„ ì°¾ì§€ ëª»í–ˆì–´ìš”. ì‹œê°„/êµ¬ê°„ì´ í¬í•¨ëœ ì„¼ì„œ ì§ˆë¬¸ì„ ë¨¼ì € í•´ì£¼ì„¸ìš”.)"

    return _format_full_rows(
        rows=LAST_SENSOR_CTX["rows"],
        start=LAST_SENSOR_CTX["start"],
        end=LAST_SENSOR_CTX["end"],
        tag=LAST_SENSOR_CTX["tag"] or "D?",
        label=LAST_SENSOR_CTX["label"] or "ìš”ì²­ êµ¬ê°„"
    )

def _collect_raw_rows_for_window(top_docs, start: datetime, end: datetime) -> Tuple[List[dict], Optional[str]]:
    all_rows = []
    tag = None
    for d in top_docs:
        # ëª¨ë“  ë°ì´í„° íƒ€ì…ì„ í—ˆìš© (raw_list, houravg, minavg, mintrend, null)
        schema = d.get("schema")
        file_path = d.get("id", "").lower()

        # ë°ì´í„°ê°€ ìˆì„ ê°€ëŠ¥ì„±ì´ ìˆëŠ” íŒŒì¼ë“¤ì„ ëª¨ë‘ ì‹œë„
        if schema not in ["raw_list", "houravg", "minavg", "mintrend", None]:
            continue

        # rawdata, houravg, minavg, mintrend íŒŒì¼ë“¤ì€ ëª¨ë‘ ì²˜ë¦¬ ëŒ€ìƒ
        if not any(pattern in file_path for pattern in ["rawdata", "houravg", "minavg", "mintrend"]) and schema is None:
            continue
        rows = _load_raw_rows(d.get("json") or [])
        if not rows:
            continue
        subset = select_rows_in_range(rows, start, end)
        if subset:
            all_rows.extend(subset)
            if tag is None: tag = d.get("tag", "D?")
    all_rows.sort(key=lambda x: x["timestamp"])
    return all_rows, tag

# ===== ë³´ì¡°: íŒŒì¼ íƒìƒ‰ (ì •í™• ë§¤ì¹­) =====
def find_houravg_doc_for_hour(target_dt: datetime, max_scan: int = MAX_FILES_TO_SCAN):
    paginator = s3.get_paginator("list_objects_v2")
    pages = paginator.paginate(Bucket=S3_BUCKET_DATA, Prefix=S3_PREFIX)

    scanned = 0
    for page in pages:
        for obj in page.get("Contents", []):
            k = obj["Key"]
            if not k.lower().endswith(".json"):
                continue
            key_dt, gran = parse_time_from_key(k)
            if gran == "hour" and key_dt and \
               (key_dt.year, key_dt.month, key_dt.day, key_dt.hour) == \
               (target_dt.year, target_dt.month, target_dt.day, target_dt.hour):
                d = download_and_score_file(k, f"{target_dt}")
                if d and d.get("schema") == "houravg":
                    d["tag"] = d.get("tag","D?")
                    return d
            scanned += 1
            if scanned >= max_scan:
                break
        if scanned >= max_scan:
            break
    return None

def find_minavg_doc_for_minute(target_dt: datetime, max_scan: int = MAX_FILES_TO_SCAN):
    paginator = s3.get_paginator("list_objects_v2")
    pages = paginator.paginate(Bucket=S3_BUCKET_DATA, Prefix=S3_PREFIX)
    scanned = 0
    for page in pages:
        for obj in page.get("Contents", []):
            k = obj["Key"]
            if not k.lower().endswith(".json"):
                continue
            key_dt, gran = parse_time_from_key(k)
            if gran == "minute" and key_dt and \
               (key_dt.year, key_dt.month, key_dt.day, key_dt.hour, key_dt.minute) == \
               (target_dt.year, target_dt.month, target_dt.day, target_dt.hour, target_dt.minute):
                d = download_and_score_file(k, f"{target_dt}")
                if d and d.get("schema") == "minavg":
                    d["tag"] = d.get("tag","D?")
                    return d
            scanned += 1
            if scanned >= max_scan:
                break
        if scanned >= max_scan:
            break
    return None

# ===== í˜•ì‹í™” ìœ í‹¸ (houravg ì¶œë ¥) =====
def format_houravg_answer_from_doc(d, need_fields: set) -> str:
    tag = d.get("tag","D?")
    j = d.get("json") or {}
    av = j.get("averages", {}) or {}
    ranges = j.get("hourly_ranges", {}) or {}
    trends = j.get("trends", {}) or {}

    av_std = {"temperature": av.get("temp"),
              "humidity": av.get("hum"),
              "gas": av.get("gas")}
    rng_std = {
        "temperature": (ranges.get("temp") or {}),
        "humidity":    (ranges.get("hum") or {}),
        "gas":         (ranges.get("gas") or {}),
    }
    tr_std  = {
        "temperature": trends.get("temperature"),
        "humidity":    trends.get("humidity"),
        "gas":         trends.get("gas"),
    }

    fields = list(need_fields) if need_fields else ["temperature","humidity","gas"]

    if len(fields) == 1:
        f = fields[0]
        name = FIELD_NAME_KOR.get(f, f)
        parts = []
        if av_std.get(f) is not None:
            parts.append(f"{name} í‰ê· : {av_std[f]}")
        r = rng_std.get(f) or {}
        if r:
            parts.append(f"{name} ë²”ìœ„: [{r.get('min')}~{r.get('max')}]")
        t = tr_std.get(f)
        if t:
            cr = t.get("change_rate"); st = t.get("status")
            se = f"(ì‹œì‘ {t.get('start_value')}, ë {t.get('end_value')})" if t and t.get("start_value") is not None else ""
            parts.append(f"{name} ì¶”ì„¸: {st} {cr} {se}".strip())
        return ("\n".join(parts) if parts else f"{name}: ë°ì´í„° ì—†ìŒ") + f" [{tag}]"

    lines = ["[ì‹œê°„ ë‹¨ìœ„ ì§‘ê³„ ìš”ì•½]"]
    for f in fields:
        name = FIELD_NAME_KOR.get(f, f)
        if av_std.get(f) is not None:
            lines.append(f"{name} í‰ê· : {av_std[f]}")
        r = rng_std.get(f) or {}
        if r:
            lines.append(f"{name} ë²”ìœ„: [{r.get('min')}~{r.get('max')}]")
        t = tr_std.get(f)
        if t:
            cr = t.get("change_rate"); st = t.get("status")
            se = f"(ì‹œì‘ {t.get('start_value')}, ë {t.get('end_value')})" if t and t.get("start_value") is not None else ""
            lines.append(f"{name} ì¶”ì„¸: {st} {cr} {se}".strip())
    overall = trends.get("overall")
    if overall:
        lines.append(f"ì „ì²´ ì¶”ì„¸: {overall}")
    return "\n".join(lines) + f" [{tag}]"

def format_minavg_answer_from_doc(d, need_fields: set) -> str:
    tag = d.get("tag","D?")
    j = d.get("json") or {}

    # ì‹¤ì œ minavg ë°ì´í„° êµ¬ì¡°ì— ë§ê²Œ íŒŒì‹±
    av_std = {"temperature": j.get("mintemp"),
              "humidity": j.get("minhum"),
              "gas": j.get("mingas")}

    fields = list(need_fields) if need_fields else ["temperature","humidity","gas"]

    if len(fields) == 1:
        f = fields[0]
        name = FIELD_NAME_KOR.get(f, f)
        value = av_std.get(f)
        if value is not None:
            comment = get_friendly_comment(f, value)
            return f"í•´ë‹¹ ë¶„ì˜ {name}ëŠ” í‰ê·  **{value}**ì…ë‹ˆë‹¤. {comment} [{tag}]"
        else:
            return f"í•´ë‹¹ ë¶„ì˜ {name} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. [{tag}]"

    lines = ["**ë¶„ ë‹¨ìœ„ í™˜ê²½ ìƒíƒœ**"]
    for f in fields:
        name = FIELD_NAME_KOR.get(f, f)
        value = av_std.get(f)
        if value is not None:
            comment = get_friendly_comment(f, value)
            lines.append(f"â€¢ {name}: **{value}** {comment}")
        else:
            lines.append(f"â€¢ {name}: ë°ì´í„° ì—†ìŒ")
    return "\n".join(lines) + f" [{tag}]"

def get_friendly_comment(field: str, value: float) -> str:
    """í•„ë“œê°’ì— ë”°ë¥¸ ì¹œì ˆí•œ ì„¤ëª… ì¶”ê°€"""
    if field == "temperature":
        if value < 18:
            return "ë‹¤ì†Œ ì¶¥ë„¤ìš”. ëƒ‰ë°©ë³‘ ê±¸ë¦¬ê¸° ì‰¬ìš´ ì˜¨ë„ì—ìš”!"
        elif value < 22:
            return "ì‹œì›í•˜ê³  ì¾Œì í•´ìš”. ì´ëŒ€ë¡œ ìœ ì§€í•˜ë©´ ì¢‹ê² ì–´ìš”!"
        elif value < 26:
            return "ì ì • ì˜¨ë„ë¡œ í¸ì•ˆí•´ìš”. ì´ëŒ€ë¡œ ìœ ì§€í•˜ë©´ ì¢‹ê² ì–´ìš”!"
        elif value < 30:
            return "ì¡°ê¸ˆ ë¥ë„¤ìš”. ì—ì–´ì»¨ì„ íŠ¸ëŠ” ê²ƒì´ ì¢‹ê² ì–´ìš”!"
        else:
            return "ë§ì´ ë”ì›Œìš”. ì£¼ì˜ê°€ í•„ìš”í•´ìš”!"
    elif field == "humidity":
        if value < 30:
            return "ê±´ì¡°í•´ìš”. ìŠµë„ë¥¼ ì˜¬ë¦¬ë©´ ì¢‹ê² ì–´ìš”!"
        elif value < 50:
            return "ì¾Œì í•œ ìŠµë„ì˜ˆìš”. ì´ëŒ€ë¡œë©´ ì¢‹ê² ì–´ìš”!"
        elif value < 60:
            return "ì ì • ìŠµë„ë¡œ ì¢‹ì•„ìš”. ì´ëŒ€ë¡œë„ ê´œì°®ì•„ìš”!"
        elif value < 70:
            return "ì¡°ê¸ˆ ìŠµí•´ìš”. ì œìŠµê¸°ë¥¼ ëŒë¦¬ë©´ ì¢‹ê² ì–´ìš”!"
        else:
            return "ìŠµë„ê°€ ë§ì´ ë†’ì•„ìš”!"
    elif field == "gas":
        if value < 400:
            return "ê³µê¸°ê°€ ë§¤ìš° ê¹¨ë—í•´ìš”"
        elif value < 600:
            return "ê³µê¸° ìƒíƒœê°€ ì¢‹ì•„ìš”"
        elif value < 1000:
            return "ë³´í†µ ìˆ˜ì¤€ì´ì—ìš”"
        elif value < 1500:
            return "í™˜ê¸°ê°€ í•„ìš”í•´ìš”!"
        else:
            return "í™˜ê¸°ê°€ í•„ìš”í•´ìš”!"
    return ""

# ===== ì •í™• ëª¨ë“œ =====
def find_sensor_data_from_s3_logs(query: str) -> Optional[Dict]:
    """
    S3 ë¡œê·¸ ë°ì´í„°ì—ì„œ í•´ë‹¹ ì‹œê°„ì˜ ì„¼ì„œ ë°ì´í„°ë¥¼ ì°¾ëŠ” í•¨ìˆ˜
    """
    # ìš”ì²­ëœ ì‹œê°„ ì¶”ì¶œ
    target_dt = None
    dt_strings = extract_datetime_strings(query)
    for ds in dt_strings:
        dt = parse_dt(ds)
        if dt:
            target_dt = dt
            break

    if not target_dt:
        return None

    try:
        # S3ì—ì„œ ë¡œê·¸ íŒŒì¼ ëª©ë¡ ì¡°íšŒ (ìµœê·¼ 1000ê°œ)
        prefix = f"{CHATLOG_PREFIX}{SESSION_ID}/"
        response = s3_logs.list_objects_v2(Bucket=CHATLOG_BUCKET, Prefix=prefix, MaxKeys=1000)

        if 'Contents' not in response:
            return None

        # ê° ë¡œê·¸ íŒŒì¼ì„ í™•ì¸í•´ì„œ í•´ë‹¹ ì‹œê°„ì˜ ì„¼ì„œ ë°ì´í„° ì°¾ê¸°
        for obj in response['Contents']:
            try:
                log_response = s3_logs.get_object(Bucket=CHATLOG_BUCKET, Key=obj['Key'])
                log_data = json.loads(log_response['Body'].read().decode('utf-8'))

                # sensor_data í•„ë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                sensor_data_list = log_data.get('sensor_data', [])
                if not sensor_data_list:
                    continue

                # í•´ë‹¹ ì‹œê°„ê³¼ ì¼ì¹˜í•˜ëŠ” ì„¼ì„œ ë°ì´í„° ì°¾ê¸°
                for sensor_entry in sensor_data_list:
                    data = sensor_entry.get('data')
                    if not data:
                        continue

                    schema = sensor_entry.get('schema')
                    if schema == 'raw_list' and isinstance(data, list):
                        # raw_listì—ì„œ ì •í™•í•œ ì‹œê°„ ì°¾ê¸°
                        for row in data:
                            row_time = datetime.strptime(row['timestamp'], '%Y-%m-%d %H:%M:%S')
                            if row_time == target_dt:
                                return {
                                    'timestamp': row['timestamp'],
                                    'temperature': row.get('temperature'),
                                    'humidity': row.get('humidity'),
                                    'gas': row.get('gas'),
                                    'source': 's3_log',
                                    'log_key': obj['Key']
                                }

                    elif schema in ['minavg', 'houravg'] and isinstance(data, dict):
                        # ì§‘ê³„ ë°ì´í„°ì—ì„œ ì‹œê°„ ë‹¨ìœ„ë³„ ë§¤ì¹­
                        data_time = datetime.strptime(data['timestamp'], '%Y-%m-%d %H:%M:%S')

                        # ë¶„ ë‹¨ìœ„ ë¹„êµ (minavg) ë˜ëŠ” ì‹œê°„ ë‹¨ìœ„ ë¹„êµ (houravg)
                        if schema == 'minavg' and data_time.replace(second=0) == target_dt.replace(second=0):
                            return {
                                'timestamp': data['timestamp'],
                                'temperature': data.get('temperature'),
                                'humidity': data.get('humidity'),
                                'gas': data.get('gas'),
                                'source': 's3_log',
                                'schema': schema,
                                'log_key': obj['Key']
                            }
                        elif schema == 'houravg' and data_time.replace(minute=0, second=0) == target_dt.replace(minute=0, second=0):
                            return {
                                'timestamp': data['timestamp'],
                                'temperature': data.get('temperature'),
                                'humidity': data.get('humidity'),
                                'gas': data.get('gas'),
                                'source': 's3_log',
                                'schema': schema,
                                'log_key': obj['Key']
                            }

            except Exception:
                continue  # í•´ë‹¹ ë¡œê·¸ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ì‹œ ë‹¤ìŒìœ¼ë¡œ

        return None

    except Exception as e:
        print(f"[ì˜¤ë¥˜] S3 ë¡œê·¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def maybe_answer_from_sensor_json(query: str, top_docs):
    if not top_docs: return None

    ql = query.lower()
    want_avg = ("í‰ê· " in query) or ("average" in ql)
    want_max = ("ìµœëŒ€" in query) or ("max" in ql)
    want_min = ("ìµœì†Œ" in query) or ("min" in ql)
    want_latest = ("ìµœê·¼" in query) or ("latest" in ql)
    want_first = any(k in query for k in ["ì²˜ìŒ", "ì²«", "ì²«ë²ˆì§¸"]) or ("first" in ql) or ("start" in ql)
    want_last  = ("ë§ˆì§€ë§‰" in query) or ("ë" in query) or ("ìµœì¢…" in ql) or ("last" in ql)
    want_trend = ("ì¶”ì´" in query) or ("trend" in ql) or ("ì¦ê°" in ql) or ("ë³€í™”" in ql)
    want_minute_of = ("ë¶„ì˜" in query)

    need_fields = detect_fields_in_query(query)

    # íŒŒì‹±ëœ ë‹¨ì¼ ì‹œì 
    target_dt = None
    for ds in extract_datetime_strings(query):
        dt = parse_dt(ds)
        if dt: target_dt = dt; break

    # --- ì •í™• ë§¤ì¹­ ì „ìš© ì²˜ë¦¬: ì´ˆ/ë¶„/ì‹œ ---
    gran = requested_granularity(query)

    # (A) ì´ˆ ë‹¨ìœ„: ì •í™•íˆ ë™ì¼í•œ ìƒ˜í”Œë§Œ í—ˆìš©
    if gran == "second" and target_dt is not None:
        for d in top_docs:
            if d.get("schema") != "raw_list":
                continue
            rows = _load_raw_rows(d.get("json") or [])
            for row in rows:
                if row["timestamp"] == target_dt:
                    sel = filter_fields(row, need_fields)
                    m_rows, m_start, m_end = select_rows_in_minute(rows, target_dt)
                    _set_last_ctx(window="minute", start=m_start, end=m_end, rows=m_rows, tag=d.get("tag","D?"), label="í•´ë‹¹ ë¶„")
                    return format_point_answer(sel, target_dt, tag=d.get("tag","D?"))
        row, raw_tag = fetch_raw_exact_second_all(target_dt)
        if row:
            sel = filter_fields(row, need_fields)
            _set_last_ctx(window="second", start=target_dt, end=target_dt, rows=[row], tag=raw_tag or "D?", label="í•´ë‹¹ ì´ˆ")
            return format_point_answer(sel, target_dt, tag=raw_tag or "D?")
        return f"(ìš”ì²­í•œ {target_dt.strftime('%Y-%m-%d %H:%M:%S')}ì˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.)"

    # (B) ë¶„ ë‹¨ìœ„
    if (gran == "minute" or want_minute_of) and target_dt is not None:
        # ë¨¼ì € minavg íŒŒì¼ ì°¾ê¸°
        for d in top_docs:
            if d.get("schema") == "minavg":
                key_dt, gran_k = parse_time_from_key(d["id"])
                if gran_k == "minute" and key_dt and \
                   (key_dt.year, key_dt.month, key_dt.day, key_dt.hour, key_dt.minute) == \
                   (target_dt.year, target_dt.month, target_dt.day, target_dt.hour, target_dt.minute):
                    return format_minavg_answer_from_doc(d, need_fields)

        matched = find_minavg_doc_for_minute(target_dt)
        if matched:
            return format_minavg_answer_from_doc(matched, need_fields)

        # minavg ì—†ìœ¼ë©´ raw_listì—ì„œ í•´ë‹¹ ë¶„ ì°¾ê¸°
        w_start = target_dt.replace(second=0)
        w_end = w_start + timedelta(minutes=1) - timedelta(seconds=1)
        target_filename = f"{target_dt.strftime('%Y%m%d%H%M')}_rawdata.json"

        # ì •í™•íˆ ë§¤ì¹­ë˜ëŠ” íŒŒì¼ ë¨¼ì € ì°¾ê¸°
        for d in top_docs:
            if d.get("schema") == "raw_list" and target_filename in d.get("id", ""):
                rows = _load_raw_rows(d.get("json") or [])
                if rows:
                    minute_rows = select_rows_in_range(rows, w_start, w_end)
                    if minute_rows:
                        _set_last_ctx(window="minute", start=w_start, end=w_end, rows=minute_rows, tag=d.get("tag","D?"), label="í•´ë‹¹ ë¶„")
                        return format_window_answer(minute_rows, w_start, w_end, need_fields, tag=d.get("tag","D?"), window_name="í•´ë‹¹ ë¶„", show_samples=False)

        # ì •í™• ë§¤ì¹­ ì‹¤íŒ¨í•˜ë©´ ë‹¤ë¥¸ raw_list íŒŒì¼ë“¤ ì‹œë„
        for d in top_docs:
            if d.get("schema") == "raw_list":
                rows = _load_raw_rows(d.get("json") or [])
                if rows:
                    minute_rows = select_rows_in_range(rows, w_start, w_end)
                    if minute_rows:
                        _set_last_ctx(window="minute", start=w_start, end=w_end, rows=minute_rows, tag=d.get("tag","D?"), label="í•´ë‹¹ ë¶„")
                        return format_window_answer(minute_rows, w_start, w_end, need_fields, tag=d.get("tag","D?"), window_name="í•´ë‹¹ ë¶„", show_samples=False)

        return f"(ìš”ì²­í•œ {target_dt.strftime('%Y-%m-%d %H:%M')}ì˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.)"

    # (C) ì‹œ ë‹¨ìœ„: houravg ìš°ì„ , ì—†ìœ¼ë©´ raw ë°ì´í„°ì—ì„œ ì§‘ê³„
    if gran == "hour" and target_dt is not None:
        matched = None
        for d in top_docs:
            if d.get("schema") != "houravg":
                continue
            key_dt, gran_k = parse_time_from_key(d["id"])
            if gran_k == "hour" and key_dt and \
               (key_dt.year, key_dt.month, key_dt.day, key_dt.hour) == \
               (target_dt.year, target_dt.month, target_dt.day, target_dt.hour):
                matched = d
                break
        if not matched:
            matched = find_houravg_doc_for_hour(target_dt)

        # houravgê°€ ìˆìœ¼ë©´ ì‚¬ìš©
        if matched:
            return format_houravg_answer_from_doc(matched, need_fields)

        # houravgê°€ ì—†ìœ¼ë©´ raw ë°ì´í„°ì—ì„œ í•´ë‹¹ ì‹œê°„ëŒ€ ì§‘ê³„
        h_start = target_dt.replace(minute=0, second=0, microsecond=0)
        h_end = h_start + timedelta(hours=1) - timedelta(seconds=1)

        # raw_listì—ì„œ í•´ë‹¹ ì‹œê°„ëŒ€ ë°ì´í„° ì°¾ê¸°
        for d in top_docs:
            if d.get("schema") == "raw_list":
                rows = _load_raw_rows(d.get("json") or [])
                if rows:
                    hour_rows = select_rows_in_range(rows, h_start, h_end)
                    if hour_rows:
                        _set_last_ctx(window="hour", start=h_start, end=h_end, rows=hour_rows, tag=d.get("tag","D?"), label="í•´ë‹¹ ì‹œê°„")
                        return format_window_answer(hour_rows, h_start, h_end, need_fields, tag=d.get("tag","D?"), window_name="í•´ë‹¹ ì‹œê°„", show_samples=False)

        return f"(ìš”ì²­í•œ {target_dt.strftime('%Y-%m-%d %Hì‹œ')}ì˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.)"

    # ----- ì¼ë°˜ ì§ˆì˜/êµ¬ê°„ ì§ˆì˜ -----
    range_start, range_end = get_time_range_from_query(query)

    for d in top_docs:
        j = d.get("json")
        if j is None:
            try:
                txt = d["content"]
                start = txt.find("{"); alt_start = txt.find("[")
                if alt_start != -1 and (start == -1 or alt_start < start): start = alt_start
                end = max(txt.rfind("}"), txt.rfind("]"))
                if start != -1 and end != -1 and end > start:
                    j = json.loads(txt[start:end+1])
            except Exception:
                j = None
        schema = detect_schema(j) if j is not None else None

        if schema == "raw_list":
            rows = _load_raw_rows(j)
            if not rows: continue

            # ë¶„â†’ë¶„ êµ¬ê°„
            mm_start, mm_end = get_minute_to_minute_range(query)
            if mm_start and mm_end and mm_start < mm_end:
                cur_rows = select_rows_in_range(rows, mm_start, mm_end)
                if not cur_rows: return "(ìš”ì²­í•œ ë¶„â†’ë¶„ êµ¬ê°„ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.) " + f"[{d.get('tag','D1')}]"
                ans = format_window_answer(
                    cur_rows, mm_start, mm_end, detect_fields_in_query(query),
                    tag=d.get("tag"), window_name="ë¶„â†’ë¶„ êµ¬ê°„",
                    show_samples=want_detail_list(query)
                )
                _set_last_ctx(window="range", start=mm_start, end=mm_end, rows=cur_rows, tag=d.get("tag","D?"), label="ë¶„â†’ë¶„ êµ¬ê°„")
                return ans

    return None

# ===== ëˆ„ë½ëœ í•¨ìˆ˜ë“¤ =====

# ì „ì—­ ë³€ìˆ˜ë“¤
SESSION_ID = f"{datetime.now().strftime('%Y%m%d-%H%M%S')}-{uuid.uuid4().hex[:6]}"
TURN_ID = 0
HISTORY = []
ENABLE_CHATLOG_SAVE = True
CHATLOG_PREFIX = "chatlogs/"

# í›„ì† íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥
FOLLOWUP_TIMESTAMP = None

def set_followup_timestamp(dt: datetime):
    global FOLLOWUP_TIMESTAMP
    FOLLOWUP_TIMESTAMP = dt

def expand_followup_query_with_last_window(query: str) -> str:
    """í›„ì† ì§ˆë¬¸ í™•ì¥ (ê¸°ë³¸ êµ¬í˜„)"""
    return query

def build_prompt(query: str, context: str, history: list = None) -> str:
    """RAG í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    hist_str = ""
    if history:
        for h in history[-3:]:  # ìµœê·¼ 3ê°œ ëŒ€í™”ë§Œ
            hist_str += f"Q: {h['query']}\nA: {h['answer']}\n\n"
    
    return f"""ë‹¹ì‹ ì€ ìŠ¤ë§ˆíŠ¸í™ˆ IoT ì„¼ì„œ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì´ì „ ëŒ€í™”:
{hist_str}

ê´€ë ¨ ì„¼ì„œ ë°ì´í„°:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ìœ„ ì„¼ì„œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. ì˜¨ë„ëŠ” â„ƒ, ìŠµë„ëŠ” %, CO2ëŠ” ppm ë‹¨ìœ„ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."""

def build_general_prompt(query: str, history: list = None) -> str:
    """ì¼ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    hist_str = ""
    if history:
        for h in history[-3:]:
            hist_str += f"Q: {h['query']}\nA: {h['answer']}\n\n"
    
    return f"""ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì´ì „ ëŒ€í™”:
{hist_str}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."""

def generate_answer_with_nova(prompt: str) -> str:
    """LLMì„ ì‚¬ìš©í•´ ë‹µë³€ ìƒì„±"""
    try:
        messages = [
            {"role": "user", "content": [{"type": "text", "text": prompt}]}
        ]
        text, _ = _invoke_claude(messages, max_tokens=1024, temperature=0.3)
        return text
    except Exception as e:
        return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def save_turn_to_s3(session_id: str, turn_id: int, route: str, query: str, answer: str, top_docs: list = None):
    """S3ì— ëŒ€í™” ë¡œê·¸ ì €ì¥"""
    try:
        log_data = {
            "session_id": session_id,
            "turn_id": turn_id,
            "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "route": route,
            "query": query,
            "answer": answer,
            "top_docs": top_docs or []
        }
        
        key = f"{CHATLOG_PREFIX}{session_id}/turn_{turn_id:03d}.json"
        s3_logs.put_object(
            Bucket=CHATLOG_BUCKET,
            Key=key,
            Body=json.dumps(log_data, ensure_ascii=False, indent=2),
            ContentType='application/json'
        )
    except Exception as e:
        print(f"ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")